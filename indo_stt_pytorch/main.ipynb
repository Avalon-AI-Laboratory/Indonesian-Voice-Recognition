{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><u>Ringkasan</u></h2>\n",
    "\n",
    "Dalam proyek ini, kami mencoba menerapkan arsitektur Transformer yang terinspirasi dari paper Attention is All You Need (oleh Google Brain) dan arsitektur Whisper dari OpenAI untuk melakukan Automatic Speech Recognition dalam bahasa Indonesia. Data yang digunakan dalam proyek ini adalah Common Voice (sumber: huggingface) untuk bahasa Indonesia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Import Necessary Libraries</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.functional as F_audio\n",
    "import torchaudio.transforms as T\n",
    "from torchinfo import summary\n",
    "import seaborn as sns\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import string\n",
    "\n",
    "from PreprocessAudio import PreprocessAudio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Read and Clean the Metadatas</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metadata ini akan digunakan untuk membuat dataset. Pertama, diperlukan metadata dengan format ```audio_file_path, transcription```. Pada proyek ini, transkripsi audio diharapkan dapat menebak kata-kata yang dieja dalam file audio dengan benar, sehingga keberadaan tanda baca atau huruf kapital dapat diabaikan. Hal ini juga bersifat menguntungkan karena dapat meningkatkan akurasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  = pd.read_csv('./common_voice_id/dev.tsv', sep='\\t')\n",
    "df2 = pd.read_csv('./common_voice_id/invalidated.tsv', sep='\\t')\n",
    "df3 = pd.read_csv('./common_voice_id/other.tsv', sep='\\t')\n",
    "df4 = pd.read_csv('./common_voice_id/train.tsv', sep='\\t')\n",
    "\n",
    "df  = df[['path', 'sentence']]\n",
    "df2 = df2[['path', 'sentence']]\n",
    "df3 = df3[['path', 'sentence']]\n",
    "df4 = df4[['path', 'sentence']]\n",
    "\n",
    "df = pd.concat([df, df2, df3, df4], ignore_index=True)\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>common_voice_id_26747327.mp3</td>\n",
       "      <td>Kamu harus melakukannya, suka tidak suka.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>common_voice_id_21699230.mp3</td>\n",
       "      <td>Saya dibonceng di belakang sepeda teman.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>common_voice_id_25248896.mp3</td>\n",
       "      <td>Tom berkata dia dapat menunggu lama.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>common_voice_id_25537482.mp3</td>\n",
       "      <td>Minggu lalu terus-menerus hujan.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>common_voice_id_21195036.mp3</td>\n",
       "      <td>Saat libur musim panas tahun ini saya pergi ke...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40199</th>\n",
       "      <td>common_voice_id_25039385.mp3</td>\n",
       "      <td>Aku menyuruh adikku untuk membeli gula di warung.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40200</th>\n",
       "      <td>common_voice_id_25426706.mp3</td>\n",
       "      <td>perusahaan yang berkembang selalu diikuti deng...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40201</th>\n",
       "      <td>common_voice_id_26229445.mp3</td>\n",
       "      <td>Melepaskan yang melekat membawa ke Nirvana.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40202</th>\n",
       "      <td>common_voice_id_20954419.mp3</td>\n",
       "      <td>dia tidak pernah ke dokter gigi selama hidupnya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40203</th>\n",
       "      <td>common_voice_id_25775886.mp3</td>\n",
       "      <td>Saya kemarin membeli sepatu putih di toserba.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40204 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               path  \\\n",
       "0      common_voice_id_26747327.mp3   \n",
       "1      common_voice_id_21699230.mp3   \n",
       "2      common_voice_id_25248896.mp3   \n",
       "3      common_voice_id_25537482.mp3   \n",
       "4      common_voice_id_21195036.mp3   \n",
       "...                             ...   \n",
       "40199  common_voice_id_25039385.mp3   \n",
       "40200  common_voice_id_25426706.mp3   \n",
       "40201  common_voice_id_26229445.mp3   \n",
       "40202  common_voice_id_20954419.mp3   \n",
       "40203  common_voice_id_25775886.mp3   \n",
       "\n",
       "                                                sentence  \n",
       "0              Kamu harus melakukannya, suka tidak suka.  \n",
       "1               Saya dibonceng di belakang sepeda teman.  \n",
       "2                   Tom berkata dia dapat menunggu lama.  \n",
       "3                       Minggu lalu terus-menerus hujan.  \n",
       "4      Saat libur musim panas tahun ini saya pergi ke...  \n",
       "...                                                  ...  \n",
       "40199  Aku menyuruh adikku untuk membeli gula di warung.  \n",
       "40200  perusahaan yang berkembang selalu diikuti deng...  \n",
       "40201        Melepaskan yang melekat membawa ke Nirvana.  \n",
       "40202    dia tidak pernah ke dokter gigi selama hidupnya  \n",
       "40203      Saya kemarin membeli sepatu putih di toserba.  \n",
       "\n",
       "[40204 rows x 2 columns]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing = df[:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kevin Putra Santoso\\AppData\\Local\\Temp\\ipykernel_33144\\2679286301.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_testing['sentence'][i] = df_testing['sentence'][i].lower()\n",
      "C:\\Users\\Kevin Putra Santoso\\AppData\\Local\\Temp\\ipykernel_33144\\2679286301.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_testing['sentence'][i] = ' '.join(word.strip(string.punctuation) for word in df_testing['sentence'][i].split())\n",
      "C:\\Users\\Kevin Putra Santoso\\AppData\\Local\\Temp\\ipykernel_33144\\2679286301.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_testing['sentence'][i] = ' '.join(remove_strips(word) for word in df_testing['sentence'][i].split())\n"
     ]
    }
   ],
   "source": [
    "def remove_strips(text):\n",
    "    # Mengganti tanda hubung (-) dengan spasi dan menghapus tanda baca di awal dan akhir kata\n",
    "    cleaned_text = text.replace('“', '')\n",
    "    cleaned_text = cleaned_text.replace('”', '')\n",
    "    cleaned_text = cleaned_text.replace('-', ' ')\n",
    "    cleaned_text = cleaned_text.strip(string.punctuation)\n",
    "    return cleaned_text\n",
    "\n",
    "for i in range(len(df_testing['sentence'])):\n",
    "    # print(text)\n",
    "    df_testing['sentence'][i] = df_testing['sentence'][i].lower()\n",
    "    df_testing['sentence'][i] = ' '.join(word.strip(string.punctuation) for word in df_testing['sentence'][i].split())\n",
    "    df_testing['sentence'][i] = ' '.join(remove_strips(word) for word in df_testing['sentence'][i].split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dipersiapkan sebuah direktori untuk menyimpan audio yang akan diolah dan nantinya akan dikelompokkan dalam data train dan validasi."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "HOME = os.getcwd()\n",
    "for i in range(len(df_testing)):\n",
    "    os.system(f'copy \\\"{HOME}\\\\common_voice_id\\\\clips\\\\{df_testing[\"path\"][i]}\\\" \\\"{HOME}\\\\audio_folder\\\\{df_testing[\"path\"][i]}\\\"')\n",
    "    label_list.append(df_testing['sentence'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Banyaknya label: 500\n",
      "Sampel 10 label\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['kamu harus melakukannya suka tidak suka',\n",
       " 'saya dibonceng di belakang sepeda teman',\n",
       " 'tom berkata dia dapat menunggu lama',\n",
       " 'minggu lalu terus menerus hujan',\n",
       " 'saat libur musim panas tahun ini saya pergi ke laut dan mendaki gunung',\n",
       " 'dia memanggil namanya',\n",
       " 'saat berada di sana saya belajar bahasa inggris',\n",
       " 'di mana kamu membeli buku itu',\n",
       " 'sepuluh tahun adalah waktu yang lama untuk menunggu',\n",
       " 'di atas meja ada vas bunga']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f'Banyaknya label: {len(label_list)}')\n",
    "print('Sampel 10 label')\n",
    "label_list[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset yang akan kita latih diharapkan memiliki format sebagai berikut.\n",
    "\n",
    "```python\n",
    "[[tensor_1], [transcription_1],\n",
    " [tensor_2], [transcription_2],\n",
    " ...\n",
    " [tensor_n], [transcription_n]]\n",
    "```\n",
    "\n",
    "Untuk itu modul ```Dataset``` oleh PyTorch dapat digunakan untuk membuat dataset ini. Modul ini dipanggil dengan syntax\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset\n",
    "```\n",
    "\n",
    "dengan ukuran batch (batch size) sebesar 64.\n",
    "\n",
    "tensor_i adalah tensor yang memuat matriks MFCC dari sebuah audio. Meninjau ulang bahwa sebuah matriks MFCC memiliki ukuran ```(n_mfcc, timesteps)``` dengan ```n_mfcc=64``` dan timesteps bergantung dari audio yang memiliki durasi terpanjang (timesteps tidak sama dengan durasi audio).\n",
    "\n",
    "transcription_i adalah tensor yang memuat transkripsi yang telah di encode menjadi angka dalam dictionary encoder yang ditentukan oleh user (biasa disebut sebagai vocabulary). tensor ini memiliki ukuran ```(1, max_len)``` dimana ```max_len``` adalah transkripsi terpanjang dari sebuah audio. Perlu diingat bahwa panjang transkripsi maksimum akan dibatasi sebesar 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = 'abcdefghijklmnopqrstuvwxyz '\n",
    "max_timestamps = 2752\n",
    "max_len = 256\n",
    "\n",
    "alphabets = ['', ' '] + [chr(i + 96) for i in range(1, 27)]\n",
    "char2num_dict, num2char_dict = {}, {}\n",
    "\n",
    "for index, chars in enumerate(alphabets):\n",
    "    char2num_dict[chars] = index\n",
    "    num2char_dict[index] = chars\n",
    "\n",
    "def conv_char2num(label, maxlen=max_len):\n",
    "    label = label[:maxlen].lower()\n",
    "    label_enc = []\n",
    "    padding_len = maxlen - len(label)\n",
    "    for i in label:\n",
    "        label_enc.append(char2num_dict[i])\n",
    "    return np.array(label_enc + [0] * padding_len)\n",
    "\n",
    "def conv_num2char(num):\n",
    "    txt = \"\"\n",
    "    for i in num:\n",
    "        if i == 0:\n",
    "            break\n",
    "        else:\n",
    "            txt += num2char_dict[i]\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20  2 26  2  1  3  6 19  2 15  8 12  2 21  1 12  6  1 20  6 12 16 13  2\n",
      "  9  1  5 10  1 17  2  8 10  1  9  2 19 10  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "example_text = 'Saya berangkat ke sekolah di pagi hari'\n",
    "print(conv_char2num(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df, test_size=0.8):\n",
    "    data_size = len(df)\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    split = int(test_size * data_size)\n",
    "    df_train, df_valid = df[:split], df[split:]\n",
    "    df_train\n",
    "    \n",
    "    return df_train, df_valid.reset_index(drop=True)\n",
    "\n",
    "df_train, df_valid = split_data(df_testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy train audio data to 'train' folder\n",
    "label_list_train = []\n",
    "\n",
    "for i in range(len(df_train)):\n",
    "    os.system(f'move \\\"{HOME}\\\\audio_folder\\\\{df_train[\"path\"][i]}\\\" \\\"{HOME}\\\\audio_folder\\\\train\\\\{df_train[\"path\"][i]}\\\"')\n",
    "    label_list_train.append(df_train['sentence'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy train audio data to 'valid' folder\n",
    "label_list_valid = []\n",
    "\n",
    "for i in range(len(df_valid)):\n",
    "    os.system(f'move \\\"{HOME}\\\\audio_folder\\\\{df_valid[\"path\"][i]}\\\" \\\"{HOME}\\\\audio_folder\\\\valid\\\\{df_valid[\"path\"][i]}\\\"')\n",
    "    label_list_valid.append(df_valid['sentence'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "PipelineTrain = PreprocessAudio('./audio_folder/train/', df_train, 35)\n",
    "PipelineValid = PreprocessAudio('./audio_folder/valid/', df_valid, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted audio directory at: ./audio_folder/train/\n",
      "Counter di 35\n",
      "Error di file ./audio_folder/train/common_voice_id_21194463.mp3\n",
      "Counter di 90\n",
      "Error di file ./audio_folder/train/common_voice_id_21192699.mp3\n",
      "Counter di 91\n",
      "Error di file ./audio_folder/train/common_voice_id_26237570.mp3\n",
      "Counter di 135\n",
      "Error di file ./audio_folder/train/common_voice_id_20847480.mp3\n",
      "Counter di 207\n",
      "Error di file ./audio_folder/train/common_voice_id_25469455.mp3\n",
      "Counter di 252\n",
      "Error di file ./audio_folder/train/common_voice_id_26242833.mp3\n",
      "Counter di 293\n",
      "Error di file ./audio_folder/train/common_voice_id_19783809.mp3\n",
      "Counter di 351\n",
      "Error di file ./audio_folder/train/common_voice_id_25470004.mp3\n",
      "Counter di 368\n",
      "Error di file ./audio_folder/train/common_voice_id_21699467.mp3\n",
      "Counter di 375\n",
      "Error di file ./audio_folder/train/common_voice_id_35338065.mp3\n",
      "Mounted audio directory at: ./audio_folder/valid/\n",
      "Counter di 29\n",
      "Error di file ./audio_folder/valid/common_voice_id_21587706.mp3\n",
      "Counter di 69\n",
      "Error di file ./audio_folder/valid/common_voice_id_21194346.mp3\n"
     ]
    }
   ],
   "source": [
    "dataset_train, df_train_filtered = PipelineTrain.load_audio()\n",
    "dataset_valid, df_valid_filtered = PipelineValid.load_audio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_padding(mfcc_tensor, index, n_mfcc=64, max_padding=512):\n",
    "    height, width = np.array(mfcc_tensor[index][0]).shape[0], np.array(mfcc_tensor[index][0]).shape[1]\n",
    "    \n",
    "    padded_mfcc = np.zeros([max_padding, n_mfcc])\n",
    "    padded_mfcc[:height, :width] = mfcc_tensor[index][0]\n",
    "    return padded_mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_list = []\n",
    "for i in range(len(dataset_train)):\n",
    "    train_dataset_list.append(add_padding(dataset_train, i, max_padding=max_timestamps).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_dataset_list = []\n",
    "for i in range(len(dataset_valid)):\n",
    "    valid_dataset_list.append(add_padding(dataset_valid, i, max_padding=max_timestamps).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_list = torch.tensor(train_dataset_list)\n",
    "valid_dataset_list = torch.tensor(valid_dataset_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "train_x = train_dataset_list.to(device)\n",
    "valid_x = valid_dataset_list.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>common_voice_id_35291086.mp3</td>\n",
       "      <td>pesawat ini sangat jelek</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>common_voice_id_20957324.mp3</td>\n",
       "      <td>tom menikahi perempuan yang lebih mudah dari d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>common_voice_id_35277617.mp3</td>\n",
       "      <td>untuk alasan inilah saya tidak bisa datang ber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>common_voice_id_25970214.mp3</td>\n",
       "      <td>istana persegi panjang mengelilingi taman tama...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>common_voice_id_25415776.mp3</td>\n",
       "      <td>dia suka mendengarkan musik</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           path  \\\n",
       "0  common_voice_id_35291086.mp3   \n",
       "1  common_voice_id_20957324.mp3   \n",
       "2  common_voice_id_35277617.mp3   \n",
       "3  common_voice_id_25970214.mp3   \n",
       "4  common_voice_id_25415776.mp3   \n",
       "\n",
       "                                            sentence  \n",
       "0                           pesawat ini sangat jelek  \n",
       "1  tom menikahi perempuan yang lebih mudah dari d...  \n",
       "2  untuk alasan inilah saya tidak bisa datang ber...  \n",
       "3  istana persegi panjang mengelilingi taman tama...  \n",
       "4                        dia suka mendengarkan musik  "
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_filtered.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y, valid_y = [], []\n",
    "\n",
    "for text in df_train_filtered['sentence']:\n",
    "    train_y.append(conv_char2num(text))\n",
    "\n",
    "for text in df_valid_filtered['sentence']:\n",
    "    valid_y.append(conv_char2num(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = torch.tensor(train_y)\n",
    "valid_y = torch.tensor(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class myDataset(Dataset):\n",
    "    def __init__(self, data, transcriptions):\n",
    "        self.data = data\n",
    "        self.transcriptions = transcriptions\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        mfcc_matrix = self.data[index]\n",
    "        transcription = self.transcriptions[index]\n",
    "        return mfcc_matrix, transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_dataset = myDataset(train_x, train_y)\n",
    "my_valid_dataset = myDataset(valid_x, valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_set = DataLoader(my_train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_set = DataLoader(my_valid_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Build Transformer Model </h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        error_const = torch.erf(x / math.sqrt(2.0))\n",
    "        x = x * 0.5 * (1.0 + error_const)\n",
    "        return x\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, num_vocab=30, num_hid=64):\n",
    "        super().__init__()\n",
    "        self.num_vocab = num_vocab\n",
    "        self.num_hid = num_hid\n",
    "\n",
    "    def forward(self, x):\n",
    "        maxlen = x.shape[-1]\n",
    "        pos = torch.arange(0, maxlen, 1)\n",
    "        emb = nn.Embedding(self.num_vocab, self.num_hid)(x)\n",
    "        pos_emb = nn.Embedding(maxlen, self.num_hid)(x)\n",
    "        out = emb + pos_emb\n",
    "        return out.to(device)\n",
    "\n",
    "class SpeechFeatureEmbedding(nn.Module):\n",
    "    def __init__(self, num_hid=64, maxlen=100):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=num_hid, out_channels=num_hid, kernel_size=11, stride=2, padding=5)\n",
    "        self.conv2 = nn.Conv1d(in_channels=num_hid, out_channels=num_hid, kernel_size=11, stride=2, padding=5)\n",
    "        self.conv3 = nn.Conv1d(in_channels=num_hid, out_channels=num_hid, kernel_size=11, stride=2, padding=5)\n",
    "        self.admaxpool1 = nn.AdaptiveMaxPool1d(1024)\n",
    "        self.admaxpool2 = nn.AdaptiveMaxPool1d(512)\n",
    "        self.gelu = GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x.permute(0, 2, 1))\n",
    "        x = self.gelu(x)\n",
    "        x = self.admaxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.gelu(x)\n",
    "        return x.permute(0, 2, 1)\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, rate=0.1):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.att = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, feed_forward_dim),     \n",
    "            GELU(),\n",
    "            nn.Linear(feed_forward_dim, embed_dim)\n",
    "        )\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.dropout1 = nn.Dropout(rate)\n",
    "        self.dropout2 = nn.Dropout(rate)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs, inputs)[0]\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        out = self.layernorm2(out1 + ffn_output)\n",
    "        return out\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, feed_forward_dim, dropout_rate=0.1):\n",
    "        super().__init__()\n",
    "        self.layernorm1 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.layernorm2 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.layernorm3 = nn.LayerNorm(embed_dim, eps=1e-6)\n",
    "        self.self_att = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.enc_att = nn.MultiheadAttention(embed_dim, num_heads)\n",
    "        self.self_dropout = nn.Dropout(0.5)\n",
    "        self.enc_dropout = nn.Dropout(0.1)\n",
    "        self.ffn_dropout = nn.Dropout(0.1)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, feed_forward_dim),\n",
    "            GELU(),\n",
    "            nn.Linear(feed_forward_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    def causalAttentionMask(self, size, dtype=float('-inf'), device='cpu'):\n",
    "        return torch.triu(torch.full((size, size), dtype, device=device), diagonal=1)\n",
    "\n",
    "    def forward(self, enc_out, target):\n",
    "        causal_mask = self.causalAttentionMask(size=target.shape[0], dtype=float('-inf'), device=target.device)\n",
    "        target_att = self.self_att(target, target, target, attn_mask=causal_mask, is_causal=True)[0]\n",
    "        target_norm = self.layernorm1(target + self.self_dropout(target_att))\n",
    "        enc_out = self.enc_att(target_norm, target_norm, enc_out)[0]\n",
    "        enc_out_norm = self.layernorm2(self.enc_dropout(enc_out) + target_norm)\n",
    "        ffn_out = self.ffn(enc_out_norm)\n",
    "        ffn_out_norm = self.layernorm3(enc_out_norm + self.ffn_dropout(ffn_out))\n",
    "        return ffn_out_norm\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_hid = 64,\n",
    "        num_head = 2,\n",
    "        num_feed_forward = 128,\n",
    "        source_maxlen = 100,\n",
    "        target_maxlen = 100,\n",
    "        num_layers_enc = 4,\n",
    "        num_layers_dec = 4,\n",
    "        num_classes = 30\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.num_layers_enc = num_layers_enc\n",
    "        self.num_layers_dec = num_layers_dec\n",
    "        self.target_maxlen = target_maxlen\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.enc_input = SpeechFeatureEmbedding(num_hid=num_hid, maxlen=source_maxlen)\n",
    "        self.dec_input = TokenEmbedding(num_vocab=num_classes, num_hid=num_hid)\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            self.enc_input,\n",
    "            *[TransformerEncoder(num_hid, num_head, num_feed_forward) for _ in range(num_layers_enc)]\n",
    "        )\n",
    "        \n",
    "        for i in range(num_layers_dec):\n",
    "            self.add_module(\n",
    "                f\"dec_layer_{i}\",\n",
    "                TransformerDecoder(num_hid, num_head, num_feed_forward),\n",
    "            )\n",
    "\n",
    "        self.classifier = nn.Linear(num_hid, num_classes)\n",
    "\n",
    "    def decoder(self, enc_out, target):\n",
    "        y = self.dec_input(target)\n",
    "        for i in range(self.num_layers_dec):\n",
    "            dec_layer = getattr(self, f\"dec_layer_{i}\")\n",
    "            y = dec_layer(enc_out, y)\n",
    "        return y\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        source = inputs[0].unsqueeze(0)\n",
    "        target = inputs[1].unsqueeze(0)\n",
    "        x = self.encoder(source)\n",
    "        y = self.decoder(x, target)\n",
    "        return self.classifier(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output SFE: torch.Size([64, 256, 64])\n",
      "Output TransEnc torch.Size([64, 256, 64])\n",
      "Output Token Embedding torch.Size([64, 256, 64])\n",
      "Output TransDec torch.Size([64, 256, 64])\n",
      "Output NN torch.Size([64, 256, 30])\n",
      "Output Softmax torch.Size([64, 256, 30])\n"
     ]
    }
   ],
   "source": [
    "test_dataset = my_train_dataset[:64]\n",
    "x = test_dataset[0]\n",
    "y = test_dataset[1]\n",
    "\n",
    "SFE = SpeechFeatureEmbedding().to(\"cuda:0\")\n",
    "out_sfe = SFE(x)\n",
    "print(\"Output SFE:\", out_sfe.shape)\n",
    "TransEnc = TransformerEncoder(64, 2, 128).to(\"cuda:0\")\n",
    "out_trans_enc = TransEnc(out_sfe)\n",
    "print(\"Output TransEnc\", out_trans_enc.shape)\n",
    "\n",
    "TE = TokenEmbedding().to(\"cuda:0\")\n",
    "out_te = TE(y)\n",
    "print(\"Output Token Embedding\", out_te.shape)\n",
    "TransDec = TransformerDecoder(64, 2, 128).to(\"cuda:0\")\n",
    "out_trans_dec = TransDec(out_trans_enc, out_te)\n",
    "print(\"Output TransDec\", out_trans_dec.shape)\n",
    "\n",
    "ffn = nn.Linear(64, 30).to(\"cuda:0\")\n",
    "out_ffn = ffn(out_trans_dec)\n",
    "print(\"Output NN\", out_ffn.shape)\n",
    "\n",
    "softmax = nn.functional.softmax(out_ffn, dim=-1)\n",
    "print(\"Output Softmax\", softmax.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, enc_out, layer):\n",
    "        self.enc_out = enc_out\n",
    "        self.layer = layer\n",
    "        for i in range(layer):\n",
    "            self.add_module(\n",
    "                f\"dec_layer{i}\",\n",
    "                TransformerDecoder(64, 2, 128)\n",
    "            )\n",
    "    \n",
    "    def decoder(self, enc_out, target):\n",
    "        y = self.dec_input(target)\n",
    "        for i in range(self.layer):\n",
    "            dec_layer = getattr(self, f\"dec_layer_{i}\")\n",
    "            y = dec_layer(enc_out, y)\n",
    "        return y\n",
    "\n",
    "    def forward(self, enc_out, target):\n",
    "        out = decoder(enc_out, target)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = nn.Sequential(\n",
    "            SFE,\n",
    "            *[TransformerEncoder(64, 2, 128) for _ in range(10)]\n",
    "        ).to(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_encoder = encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = TransDec(out_encoder, TE(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 64])"
      ]
     },
     "execution_count": 611,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 64])"
      ]
     },
     "execution_count": 532,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out2 = TransformerEncoder(embed_dim=64, num_heads=2, feed_forward_dim=128).to(device)(out1)\n",
    "out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 64])"
      ]
     },
     "execution_count": 533,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_y = TokenEmbedding().to(device)(test_data[1])\n",
    "out_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 64])"
      ]
     },
     "execution_count": 542,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = nn.Sequential(\n",
    "            SpeechFeatureEmbedding(),\n",
    "            *[TransformerEncoder(64, 2, 128) for _ in range(2)]\n",
    "        ).to(device)\n",
    "out_enc = encoder(test_data[0])\n",
    "out_enc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256, 64])"
      ]
     },
     "execution_count": 553,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_y2 = TransformerDecoder(64, 2, 128).to(device)(out_enc, out_y)\n",
    "out_y2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_testing = nn.Linear(64, 30).to(device)(out_y2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2752, 64])"
      ]
     },
     "execution_count": 516,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_train_dataset[:1][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Transformer()\n",
    "model.to(device)\n",
    "output_test = model(my_train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 30])"
      ]
     },
     "execution_count": 507,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([256, 30])"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = test_data[0]\n",
    "target = test_data[1]\n",
    "\n",
    "dec_input = target[:, :-1]\n",
    "dec_target = target[:, 1:]\n",
    "\n",
    "# def train_step(self, batch):\n",
    "#     \"\"\"Processes one batch inside the training loop.\"\"\"\n",
    "#     source = batch[\"source\"]\n",
    "#     target = batch[\"target\"]\n",
    "#     dec_input = target[:, :-1]\n",
    "#     dec_target = target[:, 1:]\n",
    "\n",
    "#     self.optimizer.zero_grad()\n",
    "\n",
    "#     preds = self([source, dec_input])\n",
    "#     one_hot = torch.nn.functional.one_hot(dec_target, num_classes=self.num_classes)\n",
    "#     mask = dec_target != 0\n",
    "#     loss = self.compiled_loss(preds, one_hot.float(), reduction='none')\n",
    "#     loss = (loss * mask).sum() / mask.sum()\n",
    "\n",
    "#     loss.backward()\n",
    "#     self.optimizer.step()\n",
    "\n",
    "#     self.loss_metric.update(loss.item())\n",
    "\n",
    "#     return {\"loss\": self.loss_metric.result()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 255])"
      ]
     },
     "execution_count": 562,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 255, 256])"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot = torch.zeros(dec_target.size(0), dec_target.size(1), 256)\n",
    "one_hot = one_hot.to(dec_target.device)  # Ensure the tensor is on the same device as dec_target\n",
    "indices = dec_target.unsqueeze(2).long()  # Convert indices to long data type\n",
    "one_hot.scatter_(2, indices, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = dec_target != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 255])"
      ]
     },
     "execution_count": 568,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 520,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[520], line 42\u001b[0m\n\u001b[0;32m     40\u001b[0m epoch_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m batch_src, batch_tgt \u001b[39min\u001b[39;00m train_set:  \u001b[39m# Use your training data loader\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     loss \u001b[39m=\u001b[39m train_step(batch_src, batch_tgt)\n\u001b[0;32m     43\u001b[0m     epoch_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[0;32m     45\u001b[0m avg_epoch_loss \u001b[39m=\u001b[39m epoch_loss \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(train_dataloader)\n",
      "Cell \u001b[1;32mIn[520], line 18\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(batch_src, batch_tgt)\u001b[0m\n\u001b[0;32m     15\u001b[0m targets \u001b[39m=\u001b[39m batch_tgt\u001b[39m.\u001b[39mcontiguous()\n\u001b[0;32m     17\u001b[0m \u001b[39m# Run the model with both source and target inputs\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m outputs \u001b[39m=\u001b[39m model((batch_src, batch_tgt[:, :\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]))  \u001b[39m# Use both source and target for the model input\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m# Calculate the loss\u001b[39;00m\n\u001b[0;32m     21\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m256\u001b[39m), targets[:, \u001b[39m1\u001b[39m:]\u001b[39m.\u001b[39mcontiguous()\u001b[39m.\u001b[39mview(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Kevin Putra Santoso\\anaconda3\\envs\\avalon_pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[517], line 141\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    139\u001b[0m source \u001b[39m=\u001b[39m inputs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m    140\u001b[0m target \u001b[39m=\u001b[39m inputs[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[1;32m--> 141\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(source)\n\u001b[0;32m    142\u001b[0m y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(x, target)\n\u001b[0;32m    143\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclassifier(y)\n",
      "File \u001b[1;32mc:\\Users\\Kevin Putra Santoso\\anaconda3\\envs\\avalon_pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\Kevin Putra Santoso\\anaconda3\\envs\\avalon_pytorch\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    218\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Kevin Putra Santoso\\anaconda3\\envs\\avalon_pytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[517], line 35\u001b[0m, in \u001b[0;36mSpeechFeatureEmbedding.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m---> 35\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv1(x\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m))\n\u001b[0;32m     36\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgelu(x)\n\u001b[0;32m     37\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39madmaxpool1(x)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: permute(sparse_coo): number of dimensions in the tensor input does not match the length of the desired ordering of dimensions i.e. input.dim() = 4 is not equal to len(dims) = 3"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define your model, loss function, and optimizer\n",
    "model = Transformer().to(device)  # Initialize your Transformer model\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Use Mean Squared Error loss for ASR\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# Training loop\n",
    "def train_step(batch_src, batch_tgt):\n",
    "    \n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    targets = batch_tgt.contiguous()\n",
    "    outputs = model((batch_src, batch_tgt[:, :-1]))  # Use both source and target for the model input\n",
    "    \n",
    "    loss = criterion(outputs.contiguous().view(-1, 256), targets[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "def validate():\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch_src, batch_tgt in test_set:  # Use your testing/validating data loader\n",
    "            outputs = model((batch_src, batch_tgt[:, :-1]))\n",
    "            loss = criterion(outputs, batch_tgt[:, 1:].contiguous())\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_dataloader)  # Calculate average loss\n",
    "\n",
    "num_epochs = 10  # Set the number of training epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for batch_src, batch_tgt in train_set:  # Use your training data loader\n",
    "        loss = train_step(batch_src, batch_tgt)\n",
    "        epoch_loss += loss\n",
    "\n",
    "    avg_epoch_loss = epoch_loss / len(train_dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Avg Loss: {avg_epoch_loss:.4f}\")\n",
    "\n",
    "    validation_loss = validate()\n",
    "    print(f\"Validation Loss: {validation_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avalon_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
